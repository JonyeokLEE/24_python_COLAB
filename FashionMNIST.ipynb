{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1--c3aroSCM7v_ZJNWRGMN0zjibzbLWst",
      "authorship_tag": "ABX9TyNb0tIPkhh+R2/58+WCCTX7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonyeokLEE/24_python_COLAB/blob/main/FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84oEPzL04uMv",
        "outputId": "86ba1eb6-7a76-4f38-e838-431499308838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 12.9209, Accuracy: 59.74%\n",
            "Epoch 2/10, Loss: 0.8040, Accuracy: 73.23%\n",
            "Epoch 3/10, Loss: 0.4710, Accuracy: 73.48%\n",
            "Epoch 4/10, Loss: 0.3148, Accuracy: 77.16%\n",
            "Epoch 5/10, Loss: 0.2610, Accuracy: 76.68%\n",
            "Epoch 6/10, Loss: 0.1529, Accuracy: 77.32%\n",
            "Epoch 7/10, Loss: 0.1338, Accuracy: 76.89%\n",
            "Epoch 8/10, Loss: 0.0718, Accuracy: 77.42%\n",
            "Epoch 9/10, Loss: 0.0593, Accuracy: 76.92%\n",
            "Epoch 10/10, Loss: 0.0338, Accuracy: 78.19%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/Dataset'):\n",
        "    for filename in filenames:\n",
        "        os.path.join(dirname, filename)\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "BatchSize  = 20\n",
        "learning_rate = 0.0008 #0.0002\n",
        "num_epoch = 10\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Dataset/fashion-mnist_train.csv', header = 0, index_col = 0)\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Dataset/fashion-mnist_test.csv', header = 0, index_col = 0)\n",
        "\n",
        "x_train = df_train.drop('label', axis = 1).values.reshape(-1, 28, 28)\n",
        "y_train = df_train['label'].values\n",
        "\n",
        "x_test = df_test.drop('label', axis = 1).values.reshape(-1, 28, 28)\n",
        "y_test = df_test['label'].values\n",
        "\n",
        "class CNN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_Model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 600)\n",
        "        self.fc2 = nn.Linear(600, 10)  # 10 classes in Fashion-MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN_Model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float).unsqueeze(1)  # Add channel dimension\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size= BatchSize, shuffle=True)\n",
        "\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float).unsqueeze(1)  # Add channel dimension\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size= BatchSize, shuffle=False)\n",
        "\n",
        "for epoch in range(num_epoch):  # Note: it should be num_epoch, not num_epochs\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Calculate loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch {epoch+1}/{num_epoch}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%')"
      ]
    }
  ]
}